{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature for s2s\n",
    "- https://medium.com/datadriveninvestor/attention-in-rnns-321fbcd64f05\n",
    "- https://towardsdatascience.com/light-on-math-ml-attention-with-keras-dc8dbc1fad39\n",
    "- https://machinelearningmastery.com/lstm-autoencoders/\n",
    "- https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346\n",
    "- https://machinelearningmastery.com/encoder-decoder-attention-sequence-to-sequence-prediction-keras/\n",
    "- https://towardsdatascience.com/time-series-forecasting-with-deep-learning-and-attention-mechanism-2d001fc871fc\n",
    "- https://www.angioi.com/time-series-encoder-decoder-tensorflow/\n",
    "- https://towardsdatascience.com/autoencoders-in-keras-273389677c20\n",
    "- https://iq.opengenus.org/applications-of-autoencoders/\n",
    "- https://stackoverflow.com/questions/54928981/split-autoencoder-on-encoder-and-decoder-keras\n",
    "- https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/\n",
    "- https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "- https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/\n",
    "- https://datascience.stackexchange.com/questions/25411/what-exactly-is-the-input-of-decoder-in-autoencoder-setup\n",
    "- https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "- https://towardsdatascience.com/sequence-models-by-andrew-ng-11-lessons-learned-c62fb1d3485b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import ndimage \n",
    "tf.compat.v1.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------- Utility Functions -------------------------------------------\n",
    "\"\"\" Creates subsequences of the original sequence to fit LSTM structure\n",
    " \n",
    "Args:\n",
    "    sequence_1: the first sequence which gets converted into multiple subarrays of length: n_steps\n",
    "    sequence_2: the second sequence, each n_steps'th element will be part of the output array\n",
    "    n_steps: the amount of time steps used as an input into the LSTM for prediction\n",
    "\n",
    "Returns:\n",
    "    A tuple of 2 numpy arrays in the required format\n",
    "    \n",
    "    X.shape = (X.shape[0] - n_steps, n_steps)\n",
    "    y.shape = (X.shape[0] - n_steps, 1)\n",
    "\n",
    "\"\"\"\n",
    "def subsequences(sequence_X, sequence_y, n_steps):\n",
    "    if n_steps > len(sequence_X):\n",
    "        raise Exception('subsequences: n_steps should not exceed the sequence length')\n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence_X)):\n",
    "        end_ix = i + n_steps\n",
    "\n",
    "        if end_ix > len(sequence_X):\n",
    "            break\n",
    "\n",
    "        X.append(sequence_X[i:end_ix])\n",
    "        y.append(sequence_y[end_ix-1])\n",
    "        \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "\"\"\" Subsample array to decrease the amount of data\n",
    "\n",
    "Args:\n",
    "    sequence: the input array to be subsampled\n",
    "    d_sample: sample frequency, meaning every d_sample'th element will be part of the output\n",
    "    \n",
    "Returns:\n",
    "    The subsampled array\n",
    "\n",
    "\"\"\"\n",
    "def subsample(sequence, d_sample):\n",
    "    return sequence[::d_sample]\n",
    "\n",
    "\n",
    "\"\"\" Smooth array to decrease measurement noise\n",
    "\n",
    "Args: \n",
    "    sequence: the input array to be smoothed\n",
    "    sigma: parameter for the gauss filtering\n",
    "\n",
    "Returns:\n",
    "    The smoothed array\n",
    "\"\"\"\n",
    "def smooth(sequence, sigma):\n",
    "    return ndimage.filters.gaussian_filter(sequence, sigma)\n",
    "\n",
    "\n",
    "\"\"\" Aligns two sequences\n",
    "\n",
    "    In this context this means subsampling the first array so that it afterwards has the same size as the second array\n",
    "    \n",
    "Args: \n",
    "    sequence_1: arrray to be aligned\n",
    "    sequence_2: array to be aligned to\n",
    "    \n",
    "Returns:\n",
    "    The algined array\n",
    "\"\"\"\n",
    "def align(sequence_1, sequence_2):\n",
    "    if len(sequence_1) < len(sequence_2):\n",
    "        raise Exception('align: missmatch of sequence lengths')\n",
    "    \n",
    "    sample_ratio = sequence_1.shape[0] / sequence_2.shape[0]\n",
    "\n",
    "    aligned_sequence = list()\n",
    "    for i in range(len(sequence_2)):\n",
    "        aligned_sequence.append(sequence_1[int(np.round(i * sample_ratio))])\n",
    "\n",
    "    aligned_sequence = np.array(aligned_sequence)\n",
    "    \n",
    "    return aligned_sequence\n",
    "\n",
    "\n",
    "\"\"\" Prepares the data for input into the LSTM\n",
    "\n",
    "    Preparation incudes:\n",
    "    subsampling, smoothing, aligning differnt sized sequences and reshaping the sequence to the requested format\n",
    "    \n",
    "Args:\n",
    "    input_sequence: the input feature sequence\n",
    "    label_sequence: the output/groud truth sequence\n",
    "    aligned: indicates if input and label sequence are of equal size or need alignment\n",
    "    d_sample: sample frequency\n",
    "    n_steps: the amount of time steps used as an input into the LSTM for prediction\n",
    "    sigma: parameter for the data smoothing\n",
    "\n",
    "Returns:\n",
    "    A tuple of 3 values. The prepared input sequence X, the output sequence of labels y and the scaler component for y. \n",
    "    This is needed afterwards to scale the output back to the original value range\n",
    "\"\"\"\n",
    "def prepare_data(input_sequence, label_sequence, aligned, d_sample, n_steps, sigma):\n",
    "    # align data if not of equal size\n",
    "    if not aligned:        \n",
    "        input_sequence = align(input_sequence, label_sequence)\n",
    "\n",
    "    # subsample and smooth data \n",
    "    input_sequence_ = subsample(input_sequence, d_sample)\n",
    "    input_sequence_ = smooth(input_sequence_, sigma)\n",
    "    \n",
    "    label_sequence_ = subsample(label_sequence, d_sample)\n",
    "    label_sequence_ = smooth(label_sequence_, sigma)\n",
    "\n",
    "    # convert into X and y sequences\n",
    "    X, y = subsequences(input_sequence_, label_sequence_, n_steps)\n",
    "    y = np.reshape(y, (-1, 1))\n",
    "\n",
    "    # fit and scale X\n",
    "    scaler_X = MinMaxScaler(feature_range = (0, 1))\n",
    "    scaler_X.fit(X)\n",
    "    X_scaled = scaler_X.transform(X)\n",
    "\n",
    "    # fit and scale y\n",
    "    scaler_y = MinMaxScaler(feature_range = (0, 1))\n",
    "    scaler_y.fit(y)\n",
    "    y_scaled = scaler_y.transform(y)\n",
    "\n",
    "    # reshape into correct format\n",
    "    X_scaled = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
    "    \n",
    "    return X_scaled, y_scaled, scaler_y\n",
    "\n",
    "def prepare_data_unscaled(input_sequence, label_sequence, aligned, d_sample, n_steps, sigma):\n",
    "    # align data if not of equal size\n",
    "    if not aligned:        \n",
    "        input_sequence = align(input_sequence, label_sequence)\n",
    "\n",
    "    # subsample and smooth data \n",
    "    input_sequence_ = subsample(input_sequence, d_sample)\n",
    "    input_sequence_ = smooth(input_sequence_, gauss_sigma)\n",
    "    \n",
    "    label_sequence_ = subsample(label_sequence, d_sample)\n",
    "    label_sequence_ = smooth(label_sequence_, gauss_sigma)\n",
    "\n",
    "    # convert into X and y sequences\n",
    "    X, y = subsequences(input_sequence_, label_sequence_, n_steps)\n",
    "    y = np.reshape(y, (-1, 1))\n",
    "\n",
    "    # reshape into correct format\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------- Hyperparameters -------------------------------------------\n",
    "n_steps = 50\n",
    "n_features = 2\n",
    "n_lstm_units_1 = 20\n",
    "n_lstm_units_2 = 10\n",
    "n_dense_units = 1\n",
    "n_epochs = 5\n",
    "\n",
    "d_sample = 1\n",
    "gauss_sigma = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------- Prepare Training Data -------------------------------------------\n",
    "# load data\n",
    "train_cur_inv = np.loadtxt('../data/fobss_data/data/Profile 10A/inverter/Inverter_Current.csv', delimiter=';')\n",
    "train_cur_inv = train_cur_inv[:,1]\n",
    "train_volt_master = np.loadtxt('../data/fobss_data/data/Profile 10A/battery/Battery_Voltage.csv', delimiter=';')\n",
    "train_volt_master = train_volt_master[:,1]\n",
    "train_volt_slave_0_cell_4 = np.loadtxt('../data/fobss_data/data/Profile 10A/cells/Slave_0_Cell_Voltages.csv', delimiter=';')\n",
    "train_volt_slave_0_cell_4 = train_volt_slave_0_cell_4[:,4]\n",
    "train_volt_repeat = np.full(shape=train_volt_slave_0_cell_4.shape[0], fill_value=train_volt_slave_0_cell_4[0], dtype=np.float)\n",
    "\n",
    "# prepare LSTM input\n",
    "X1_train, y1_train, scaler_y = prepare_data(train_cur_inv, train_volt_slave_0_cell_4, False, d_sample, n_steps, gauss_sigma)\n",
    "\n",
    "inv_cum = np.cumsum(train_cur_inv)\n",
    "\n",
    "X2_train, y2_train, scaler_y = prepare_data(inv_cum, train_volt_slave_0_cell_4, False, d_sample, n_steps, gauss_sigma)\n",
    "X_train = np.append(X1_train, X2_train, axis=2)\n",
    "y_train = y1_train\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# ------------------------------------------- multiple profiles -------------------------------------------\n",
    "cur_profile_1 = np.loadtxt('../data/fobss_data/data/Profile 10A/inverter/Inverter_Current.csv', delimiter=';')\n",
    "cur_profile_1 = cur_profile_1[:,1]\n",
    "volt_profil_1 = np.loadtxt('../data/fobss_data/data/Profile 10A/cells/Slave_0_Cell_Voltages.csv', delimiter=';')\n",
    "volt_profil_1 = volt_profil_1[:,4]\n",
    "\n",
    "cur_profile_2 = np.loadtxt('../data/fobss_data/data/Profile 10A/inverter/Inverter_Current.csv', delimiter=';')\n",
    "cur_profile_2 = cur_profile_2[:,1]\n",
    "volt_profil_2 = np.loadtxt('../data/fobss_data/data/Profile 10A/cells/Slave_0_Cell_Voltages.csv', delimiter=';')\n",
    "volt_profil_2 = volt_profil_2[:,4]\n",
    "\n",
    "cur_profile_3 = np.loadtxt('../data/fobss_data/data/Profile 25A/inverter/Inverter_Current.csv', delimiter=';')\n",
    "cur_profile_3 = cur_profile_3[:,1]\n",
    "volt_profil_3 = np.loadtxt('../data/fobss_data/data/Profile 25A/cells/Slave_0_Cell_Voltages.csv', delimiter=';')\n",
    "volt_profil_3 = volt_profil_3[:,4]\n",
    "\n",
    "cur_profile_4 = np.loadtxt('../data/fobss_data/data/Profile 25A/inverter/Inverter_Current.csv', delimiter=';')\n",
    "cur_profile_4 = cur_profile_4[:,1]\n",
    "volt_profil_4 = np.loadtxt('../data/fobss_data/data/Profile 25A/cells/Slave_0_Cell_Voltages.csv', delimiter=';')\n",
    "volt_profil_4 = volt_profil_4[:,4]\n",
    "\n",
    "cur_profile_1 = align(cur_profile_1, cur_profile_1)\n",
    "cur_profile_1 = cur_profile_1[:2000]\n",
    "volt_profil_1 = volt_profil_1[:2000]\n",
    "\n",
    "cur_profile_2 = align(cur_profile_2, cur_profile_2)\n",
    "cur_profile_2 = cur_profile_2[:2000]\n",
    "volt_profil_2 = volt_profil_2[:2000]\n",
    "\n",
    "cur_profile_3 = align(cur_profile_3, cur_profile_3)\n",
    "cur_profile_3 = cur_profile_3[:2000]\n",
    "volt_profil_3 = volt_profil_3[:2000]\n",
    "\n",
    "cur_profile_4 = align(cur_profile_4, cur_profile_4)\n",
    "cur_profile_4 = cur_profile_4[:2000]\n",
    "volt_profil_4 = volt_profil_4[:2000]\n",
    "\n",
    "\n",
    "X1_train, y1_train, scaler_y = prepare_data(cur_profile_1, volt_profil_1, True, d_sample, n_steps, gauss_sigma)\n",
    "X2_train, y2_train, scaler_y = prepare_data(cur_profile_2, volt_profil_2, True, d_sample, n_steps, gauss_sigma)\n",
    "X3_train, y3_train, scaler_y = prepare_data(cur_profile_3, volt_profil_3, True, d_sample, n_steps, gauss_sigma)\n",
    "X4_train, y4_train, scaler_y = prepare_data(cur_profile_4, volt_profil_4, True, d_sample, n_steps, gauss_sigma)\n",
    "\n",
    "X12_train = np.append(X1_train, X2_train, axis=0)\n",
    "X123_train = np.append(X12_train, X3_train, axis=0)\n",
    "X1234_train = np.append(X123_train, X4_train, axis=0)\n",
    "\n",
    "y12_train = np.append(y1_train, y2_train, axis=0)\n",
    "y123_train = np.append(y12_train, y3_train, axis=0)\n",
    "y1234_train = np.append(y123_train, y4_train, axis=0)\n",
    "\n",
    "# print(X1234_train.shape, y1234_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------- Model: Autoencoder -------------------------------------------\n",
    "n_past_steps = n_steps\n",
    "n_past_features = 1\n",
    "n_lstm_cells = 10\n",
    "n_fut_steps = n_steps\n",
    "n_fut_features = n_features\n",
    "\n",
    "Encoder_train, _, _ = prepare_data(train_volt_slave_0_cell_4, train_volt_slave_0_cell_4, True, d_sample, n_steps, gauss_sigma)\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(n_past_steps, n_past_features), name='encoder_inputs')\n",
    "encoder = layers.LSTM(n_lstm_cells, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(n_fut_steps, n_fut_features), name='decoder_inputs')\n",
    "decoder_lstm = layers.LSTM(n_lstm_cells, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = layers.Dense(1, activation='relu')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs)\n",
    "model.summary()\n",
    "print(X_train.shape, Encoder_train.shape, y_train.shape)\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "loss = keras.losses.Huber()\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit([Encoder_train, X_train], y_train, epochs = n_epochs, verbose = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------- inference setup ---------------------------------------------\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = keras.Input(shape=(n_lstm_cells,))\n",
    "decoder_state_input_c = keras.Input(shape=(n_lstm_cells,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "states_value = encoder_model.predict(Encoder_train)\n",
    "# output_tokens, h, c = decoder_model.predict([y_train] + states_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------ Model: LSTM ------------------------------------------------\n",
    "# model = keras.Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "# model.add(layers.LSTM(units = n_lstm_units_1, activation='relu', input_shape = (n_steps, n_features), return_sequences=True))\n",
    "\n",
    "# model.add(layers.LSTM(units = n_lstm_units_2))\n",
    "\n",
    "# Adding the output layer\n",
    "# model.add(layers.Dense(units = n_dense_units))\n",
    "\n",
    "# Show model\n",
    "# model.summary()\n",
    "\n",
    "# Compile model\n",
    "# model.compile(optimizer = 'RMSprop', loss = 'mse')\n",
    "\n",
    "# Fitting the LSTM to the Training set\n",
    "# history = model.fit(X_train, y_train, epochs = n_epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------- Visualize Training -------------------------------------------\n",
    "loss = history.history['loss']\n",
    "epochs = range(1,len(loss)+1)\n",
    "\n",
    "plt.title('Training error with epochs')\n",
    "plt.plot(epochs,loss,'-o',label='training loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('training error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------- Prepare Test Data -------------------------------------------\n",
    "# load data\n",
    "test_cur_inv = np.loadtxt('../data/fobss_data/data/Profile 10A/inverter/Inverter_Current.csv', delimiter=';')\n",
    "test_cur_inv = test_cur_inv[:,1]\n",
    "test_volt_master = np.loadtxt('../data/fobss_data/data/Profile 10A/battery/Battery_Voltage.csv', delimiter=';')\n",
    "test_volt_master = test_volt_master[:,1]\n",
    "test_volt_slave_0_cell_4 = np.loadtxt('../data/fobss_data/data/Profile 10A/cells/Slave_0_Cell_Voltages.csv', delimiter=';')\n",
    "test_volt_slave_0_cell_4 = test_volt_slave_0_cell_4[:,4]\n",
    "test_volt_repeat = np.full(shape=test_volt_slave_0_cell_4.shape[0], fill_value=test_volt_slave_0_cell_4[0], dtype=np.float)\n",
    "\n",
    "# prepare prediction\n",
    "X1_test, y_test1, scaler_y = prepare_data(test_cur_inv, test_volt_slave_0_cell_4, False, d_sample, n_steps, gauss_sigma)\n",
    "# X2_test, y_test2 = prepare_data_unscaled(test_volt_repeat, test_volt_slave_0_cell_4, False, d_sample, n_steps, gauss_sigma)\n",
    "# X_test = np.append(X1_test, X2_test, axis=2)\n",
    "# y_test_unscaled = y_test2\n",
    "\n",
    "print(X1_test.shape, y_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "yhat = model.predict([Encoder_train, X_train], verbose = 1)\n",
    "yhat.shape\n",
    "# unscale data for visualization\n",
    "# yhat_rescaled = scaler_y.inverse_transform(yhat)\n",
    "# y_test_unscaled = scaler_y.inverse_transform(y_train)\n",
    "# plot test results\n",
    "# plt.plot(yhat, color='red', label = 'predicted')\n",
    "# plt.plot(y_test1, color='blue', label = 'measured')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "battery-system",
   "language": "python",
   "name": "battery-system"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
